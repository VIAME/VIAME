# =============================================================================
# Title: Train Detector - Adaptive
#
# Description: Configuration for training object detector
# =============================================================================

#  ============================== Adaptive Detector Trainer ==============================
#
#  This configuration uses the adaptive trainer to analyze training data statistics
#  and run appropriate training pipelines.
#
#  Up to 3 trainers will be run sequentially based on:
#  - Hard requirements: minimum annotations per class, minimum object sizes,
#                       max aspect ratio variance, max density, max class imbalance
#  - Soft preferences: annotation count (low/medium/high), object size (small/medium/large),
#                      aspect ratio (tall/square/wide), density (sparse/medium/dense),
#                      scale (uniform/multi-scale), overlap (low/medium/high), masks
#
#  Statistics computed from training data:
#    - Annotation counts (total, per-class, class imbalance ratio)
#    - Object sizes (mean, median, percentiles, small/medium/large distribution)
#    - Aspect ratios (mean, std, tall/square/wide distribution)
#    - Object density (mean/max per frame, crowded/sparse frame counts)
#    - Scale variance (variance of sizes, min/max ratio, multi-scale flag)
#    - Spatial distribution (edge object fraction)
#    - Overlap/occlusion (mean IoU, overlapping pairs, high overlap fraction)
#    - Mask presence (count, fraction, has_masks flag)
#
#  Configured trainers:
#    1. SVM           - Fast classical ML, works with few annotations (50+)
#    2. MIT-YOLO      - Modern YOLO variant (v9-c), needs moderate data (200+)
#    3. Netharn CFRNN - Deep learning with ResNeXt backbone, needs more data (500+)
#    4. Netharn Grid  - Deep learning with tiling for small objects (300+)
#    5. LitDet FRCNN  - Faster R-CNN alternative, needs moderate data (300+)
#    6. RF-DETR       - Transformer-based detector, high data (400+)
#
#  ======================================================================================

#  Groundtruth file extensions (txt, kw18, etc...). Note: this is indepedent of
#  the format that's stored in the file.
groundtruth_extensions = .csv

#  Algorithm to use for 'groundtruth_reader'.
groundtruth_reader:type = viame_csv

#  Dump possible input data formatting warnings to these files
data_warning_file = TRAINING_DATA_WARNINGS.txt
groundtruth_reader:viame_csv:warning_file = TRAINING_DATA_WARNINGS.txt

#  Can be either: "one_per_file" or "one_per_folder".
groundtruth_style = one_per_folder

#  Semicolon list of seperated image extensions to use in training, images
#  without this extension will not be included.
image_extensions = .jpg;.jpeg;.JPG;.JPEG;.tif;.tiff;.TIF;.TIFF;.png;.PNG;.sgi;.SGI;.bmp;.BMP;.pgm;.PGM

#  Semicolon list of seperated video extensions to use in training, videos
#  without this extension will not be included.
video_extensions = .mp4;.MP4;.mpg;.MPG;.mpeg;.MPEG;.avi;.AVI;.wmv;.WMV;.mov;.MOV;.webm;.WEBM;.ogg;.OGG

#  Pipeline to use to extract video frames if inputs are videos
relativepath video_extractor = filter_default.pipe

#  Percent [0.0, 1.0] of validation samples to use if no manual files specified.
default_percent_validation = 0.10

#  Number of validation frames to group together in one test burst
validation_burst_frame_count = 10


# ==============================================================================
#  Algorithm to use for 'detector_trainer'.
# ==============================================================================
detector_trainer:type = adaptive

block detector_trainer:adaptive

  # Maximum number of trainers to run sequentially
  max_trainers_to_run = 3

  # ---- Object size thresholds (area in pixels^2) ----
  small_object_threshold = 1024
  large_object_threshold = 16384

  # ---- Annotation count thresholds for preference matching ----
  low_annotation_threshold = 500
  high_annotation_threshold = 2000

  # ---- Aspect ratio thresholds ----
  # Objects with aspect ratio (w/h) < tall_threshold are considered "tall"
  # Objects with aspect ratio (w/h) > wide_threshold are considered "wide"
  tall_aspect_threshold = 0.7
  wide_aspect_threshold = 1.4

  # ---- Density thresholds (objects per frame) ----
  sparse_frame_threshold = 5
  crowded_frame_threshold = 20

  # ---- Class thresholds ----
  # Classes with count < rare threshold are "rare"
  # Classes with count > dominant threshold are "dominant"
  rare_class_threshold = 50
  dominant_class_threshold = 500

  # ---- Edge detection ----
  # Fraction of image dimension for edge margin (for spatial distribution)
  edge_margin_fraction = 0.05

  # ---- Overlap threshold ----
  # IoU threshold above which objects are considered overlapping
  overlap_iou_threshold = 0.3

  # Output statistics to JSON file
  output_statistics_file = training_data_statistics.json

  # Enable verbose logging
  verbose = true

endblock


# ==============================================================================
#  Trainer 1: SVM Classifier
#  - Fast training, works with limited data
#  - Best for: Low annotation counts, quick baseline model
#  - Works well with uniform scale, doesn't need masks
# ==============================================================================
block detector_trainer:adaptive:trainer_1
  # ---- Hard requirements ----
  required_min_count_per_class = 50
  required_min_object_area = 0
  required_percentile = 0.0
  required_max_aspect_ratio_std = 0.0
  required_max_objects_per_frame = 0
  required_max_class_imbalance = 0.0
  required_masks = false

  # ---- Soft preferences ----
  annotation_count_preference = low
  object_size_preference = medium
  aspect_ratio_preference =
  density_preference = sparse
  scale_preference = uniform
  overlap_preference = low
  prefers_masks = false
endblock

detector_trainer:adaptive:trainer_1:trainer:type = svm

block detector_trainer:adaptive:trainer_1:trainer:svm
  mode = detector
  gt_frames_only = false
  relativepath pipeline_template = templates/embedded_generic_svm.pipe
  output_directory = category_models/svm
endblock


# ==============================================================================
#  Trainer 2: MIT-YOLO (v9-c)
#  - Modern YOLO variant with excellent speed/accuracy tradeoff
#  - Best for: Medium data, fast inference, handles multi-scale well
#  - Good with moderate overlap/occlusion
# ==============================================================================
block detector_trainer:adaptive:trainer_2
  # ---- Hard requirements ----
  required_min_count_per_class = 200
  required_min_object_area = 400
  required_percentile = 0.3
  required_max_aspect_ratio_std = 0.0
  required_max_objects_per_frame = 0
  required_max_class_imbalance = 50.0
  required_masks = false

  # ---- Soft preferences ----
  annotation_count_preference = medium
  object_size_preference = medium
  aspect_ratio_preference = square
  density_preference = medium
  scale_preference = multi-scale
  overlap_preference = medium
  prefers_masks = false
endblock

detector_trainer:adaptive:trainer_2:trainer:type = ocv_windowed

block detector_trainer:adaptive:trainer_2:trainer:ocv_windowed
  train_directory = deep_training/mit_yolo
  mode = disabled
  chip_width = 640
  chip_height = 640
  chip_adaptive_thresh = 1600000
  min_train_box_length = 5
  small_box_area = 200
  small_action = remove
  image_reader:type = vxl
endblock

block detector_trainer:adaptive:trainer_2:trainer:ocv_windowed:trainer
  type = mit_yolo
  mit_yolo:model = v9-c
  mit_yolo:gpu_count = -1
  mit_yolo:chip_width = 640
  relativepath mit_yolo:weight = true
  mit_yolo:batch_size = 4
  mit_yolo:max_epochs = 50
  mit_yolo:learning_rate = 1e-3
endblock


# ==============================================================================
#  Trainer 3: Netharn CFRNN (ResNeXt101 backbone)
#  - High accuracy deep learning detector
#  - Best for: High annotation counts, handles various aspect ratios well
#  - Can handle high class imbalance with augmentation
# ==============================================================================
block detector_trainer:adaptive:trainer_3
  # ---- Hard requirements ----
  required_min_count_per_class = 500
  required_min_object_area = 256
  required_percentile = 0.5
  required_max_aspect_ratio_std = 0.0
  required_max_objects_per_frame = 0
  required_max_class_imbalance = 0.0
  required_masks = false

  # ---- Soft preferences ----
  annotation_count_preference = high
  object_size_preference =
  aspect_ratio_preference =
  density_preference = medium
  scale_preference = multi-scale
  overlap_preference = medium
  prefers_masks = false
endblock

detector_trainer:adaptive:trainer_3:trainer:type = ocv_windowed

block detector_trainer:adaptive:trainer_3:trainer:ocv_windowed
  train_directory = deep_training/netharn_cfrnn
  mode = original_and_resized
  chip_width = 640
  chip_height = 640
  chip_adaptive_thresh = 1600000
  min_train_box_length = 5
  image_reader:type = vxl
endblock

block detector_trainer:adaptive:trainer_3:trainer:ocv_windowed:trainer
  type = netharn
  netharn:gpu_count = -1
  netharn:chip_width = 640
  relativepath netharn:backbone = models/pytorch_resnext101.pth
  relativepath netharn:pipeline_template = templates/embedded_netharn.pipe
  netharn:timeout = default
  netharn:batch_size = auto
  netharn:learning_rate = auto
  netharn:augmentation = complex
endblock


# ==============================================================================
#  Trainer 4: Netharn CFRNN with Grid/Tiling (for small objects)
#  - Deep learning with image scaling for small object detection
#  - Best for: Small objects that benefit from tiling, dense scenes
#  - Handles high object density well due to windowed approach
# ==============================================================================
block detector_trainer:adaptive:trainer_4
  # ---- Hard requirements ----
  required_min_count_per_class = 300
  required_min_object_area = 0
  required_percentile = 0.0
  required_max_aspect_ratio_std = 0.0
  required_max_objects_per_frame = 0
  required_max_class_imbalance = 0.0
  required_masks = false

  # ---- Soft preferences ----
  annotation_count_preference = medium
  object_size_preference = small
  aspect_ratio_preference =
  density_preference = dense
  scale_preference = uniform
  overlap_preference = high
  prefers_masks = false
endblock

detector_trainer:adaptive:trainer_4:trainer:type = ocv_windowed

block detector_trainer:adaptive:trainer_4:trainer:ocv_windowed
  train_directory = deep_training/netharn_grid
  mode = scale
  scale = 1.25
  chip_width = 640
  chip_height = 640
  min_train_box_length = 5
  image_reader:type = vxl
endblock

block detector_trainer:adaptive:trainer_4:trainer:ocv_windowed:trainer
  type = netharn
  netharn:gpu_count = -1
  netharn:chip_width = 640
  relativepath netharn:backbone = models/pytorch_resnext101.pth
  relativepath netharn:pipeline_template = templates/embedded_netharn_grid.pipe
  netharn:timeout = default
  netharn:batch_size = auto
  netharn:learning_rate = auto
  netharn:augmentation = complex
endblock


# ==============================================================================
#  Trainer 5: LitDet Faster R-CNN
#  - PyTorch Lightning-based Faster R-CNN
#  - Best for: Medium-large objects, handles various aspect ratios well
#  - Good at handling tall objects (e.g., standing animals, people)
# ==============================================================================
block detector_trainer:adaptive:trainer_5
  # ---- Hard requirements ----
  required_min_count_per_class = 300
  required_min_object_area = 900
  required_percentile = 0.4
  required_max_aspect_ratio_std = 0.0
  required_max_objects_per_frame = 0
  required_max_class_imbalance = 0.0
  required_masks = false

  # ---- Soft preferences ----
  annotation_count_preference = medium
  object_size_preference = large
  aspect_ratio_preference = tall
  density_preference = sparse
  scale_preference = multi-scale
  overlap_preference = low
  prefers_masks = false
endblock

detector_trainer:adaptive:trainer_5:trainer:type = ocv_windowed

block detector_trainer:adaptive:trainer_5:trainer:ocv_windowed
  train_directory = deep_training/litdet_frcnn
  mode = original_and_resized
  chip_width = 640
  chip_height = 640
  chip_adaptive_thresh = 1600000
  min_train_box_length = 5
  image_reader:type = vxl
endblock

block detector_trainer:adaptive:trainer_5:trainer:ocv_windowed:trainer
  type = litdet
  litdet:model_type = faster_rcnn
  litdet:backbone = resnet50_fpn
  litdet:device = auto
  litdet:fine_tune = true
  litdet:pretrained_weights = coco
  litdet:chip_width = 640
  relativepath litdet:pipeline_template = templates/embedded_litdet.pipe
  litdet:batch_size = 2
  litdet:max_epochs = 100
  litdet:learning_rate = 1e-3
  litdet:weight_decay = 1e-6
  litdet:trainable_backbone_layers = 3
  litdet:save_top_k = 1
  litdet:use_tensorboard = true
  litdet:run_test = true
endblock


# ==============================================================================
#  Trainer 6: RF-DETR (Transformer-based detector)
#  - State-of-the-art transformer architecture
#  - Best for: High data counts, handles varying aspect ratios/scales
#  - Transformer attention handles dense scenes and occlusion well
# ==============================================================================
block detector_trainer:adaptive:trainer_6
  # ---- Hard requirements ----
  required_min_count_per_class = 400
  required_min_object_area = 400
  required_percentile = 0.5
  required_max_aspect_ratio_std = 0.0
  required_max_objects_per_frame = 0
  required_max_class_imbalance = 0.0
  required_masks = false

  # ---- Soft preferences ----
  annotation_count_preference = high
  object_size_preference = medium
  aspect_ratio_preference =
  density_preference = dense
  scale_preference = multi-scale
  overlap_preference = high
  prefers_masks = false
endblock

detector_trainer:adaptive:trainer_6:trainer:type = ocv_windowed

block detector_trainer:adaptive:trainer_6:trainer:ocv_windowed
  train_directory = deep_training/rf_detr
  mode = disabled
  chip_width = 560
  chip_height = 560
  chip_adaptive_thresh = 1600000
  min_train_box_length = 5
  small_box_area = 200
  small_action = remove
  image_reader:type = vxl
endblock

block detector_trainer:adaptive:trainer_6:trainer:ocv_windowed:trainer
  type = rf_detr
  rf_detr:model_size = base
  rf_detr:device = auto
  rf_detr:chip_width = 560
  rf_detr:seed_model =
  rf_detr:batch_size = 4
  rf_detr:max_epochs = 50
  rf_detr:learning_rate = 1e-4
  rf_detr:learning_rate_encoder = 1.5e-4
  rf_detr:grad_accum_steps = 4
  rf_detr:use_ema = True
  rf_detr:ema_decay = 0.993
  rf_detr:checkpoint_interval = 10
endblock
