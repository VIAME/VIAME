# =============================================================================
# Title: Train Detector - NETHARN RF DETR
#
# Description: Configuration for training RT-DETR. Uses the NetHarn deep
# learning framework. Input resolution: 560px.
# =============================================================================

include common_train_detector.conf

#  Pipeline template file.
relativepath pipeline_template = templates/embedded_netharn_rf_detr.pipe

#  Augmentation pipeline.
#relativepath augmentation_pipeline = train_aug_hue_shifting_only.pipe

#  Augmentatation cache.
#augmentation_cache = augmented_images

#  Do not regenerate cache.
#regenerate_cache = true


#  Algorithm to use for 'detector_trainer'.
detector_trainer:type = ocv_windowed

block detector_trainer:ocv_windowed

  # Directory for all files used in training
  train_directory = deep_training

  # Windowing mode, can be disabled, maintain_ar, scale, chip, adaptive
  mode = original_and_resized

  # Resized chip width (RF-DETR default is 560).
  chip_width = 560

  # Resized chip height (RF-DETR default is 560).
  chip_height = 560

  # Adaptive size threshold for chipping
  chip_adaptive_thresh = 1600000

  # Don't train on chips with detections smaller than this
  min_train_box_length = 5

  # Uncomment to remove small detections instead of training on them
  #small_box_area = 290
  #small_action = remove

  # Image reader type
  image_reader:type = vxl

endblock

block detector_trainer:ocv_windowed:trainer

  # Trainer type
  type = netharn

  # The architecture (rf_detr, rfdetr_base, rfdetr_large, rfdetr_small, rfdetr_medium, rfdetr_nano)
  netharn:arch = rf_detr

  # Number of GPUs to use, -1 indicates all
  netharn:gpu_count = -1

  # Network perspective field size
  netharn:chip_width = 560

  # Backbone file (True for auto-download, False for none, or path to weights)
  #relativepath netharn:backbone = models/rf_detr_base.pth

  # Max timeout in seconds (example = 604800, or one week)
  netharn:timeout = default

  # Image train batch size
  netharn:batch_size = auto

  # Training learning rate
  netharn:learning_rate = auto

  # Augmentation method
  netharn:augmentation = complex

endblock
