name: Build and Release

on:
  push:
    branches:
      - main

concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: true

jobs:
  build-linux-stage1:
    name: Build Linux Stage 1 (fletch)
    runs-on: ubuntu-latest
    timeout-minutes: 480  # 8 hours for fletch
    outputs:
      cache-hit: ${{ steps.cache-stage1.outputs.cache-hit }}
      cache-key: ${{ steps.cache-key.outputs.key }}

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          submodules: true  # Need submodules to compute hashes

      - name: Compute Stage 1 cache key
        id: cache-key
        run: |
          # Compute hash of files that affect Stage 1 build (fletch only)
          FLETCH_CMAKE_HASH=$(sha256sum cmake/add_project_fletch.cmake | cut -c1-16)
          FLETCH_PKG_HASH=$(find packages/fletch -type f \( -name "*.cmake" -o -name "CMakeLists.txt" -o -name "*.patch" \) -exec sha256sum {} \; 2>/dev/null | sort | sha256sum | cut -c1-16)
          BUILD_SCRIPT_HASH=$(sha256sum cmake/build_server_rocky_stage1.sh cmake/build_server_deps_yum.sh cmake/build_server_linux_ssl.sh cmake/build_server_linux_cmake.sh 2>/dev/null | sort | sha256sum | cut -c1-16)

          CACHE_KEY="linux-stage1-${FLETCH_CMAKE_HASH}-${FLETCH_PKG_HASH}-${BUILD_SCRIPT_HASH}"
          echo "key=$CACHE_KEY" >> $GITHUB_OUTPUT
          echo "Cache key: $CACHE_KEY"

      - name: Check Stage 1 cache
        id: cache-stage1
        uses: actions/cache@v4
        with:
          path: stage1-artifacts/stage1-build.tar.zst
          key: ${{ steps.cache-key.outputs.key }}

      - name: Free disk space
        if: steps.cache-stage1.outputs.cache-hit != 'true'
        run: |
          # Remove unnecessary files to free up disk space
          sudo rm -rf /usr/share/dotnet
          sudo rm -rf /usr/local/lib/android
          sudo rm -rf /opt/ghc
          sudo rm -rf /opt/hostedtoolcache/CodeQL
          sudo rm -rf /usr/local/share/boost
          sudo rm -rf /usr/share/swift
          sudo rm -rf /usr/local/julia*
          sudo rm -rf /opt/microsoft
          sudo rm -rf /opt/az
          sudo docker image prune --all --force
          df -h

      - name: Set up Docker
        if: steps.cache-stage1.outputs.cache-hit != 'true'
        run: |
          # Ensure docker is available
          docker --version

      - name: Build Stage 1 in Rocky Linux container
        if: steps.cache-stage1.outputs.cache-hit != 'true'
        run: |
          # Pull the Rocky Linux CUDA image
          docker pull nvidia/cuda:12.6.3-cudnn-devel-rockylinux8

          # Start container (no GPU runtime needed for compilation)
          docker run -td --name viame_build nvidia/cuda:12.6.3-cudnn-devel-rockylinux8 bash

          # Copy source into container
          docker cp . viame_build:/viame/

          # Make build script executable
          docker exec viame_build chmod +x /viame/cmake/build_server_rocky_stage1.sh

          # Run the Stage 1 build script
          docker exec viame_build /viame/cmake/build_server_rocky_stage1.sh

      - name: Verify Stage 1 completed successfully
        if: steps.cache-stage1.outputs.cache-hit != 'true'
        run: |
          echo "Checking if Stage 1 build completed successfully..."

          # Check that fletch was built
          if docker exec viame_build test -d /viame/build/build/src/fletch-build; then
            echo "SUCCESS: fletch-build directory found"
          else
            echo "ERROR: fletch-build directory not found"
            exit 1
          fi

          # Check for install directory
          if docker exec viame_build test -d /viame/build/install; then
            echo "SUCCESS: install directory found"
          else
            echo "ERROR: install directory not found"
            exit 1
          fi

          echo "Stage 1 verification passed!"

      - name: Clean intermediate files before packaging
        if: steps.cache-stage1.outputs.cache-hit != 'true'
        run: |
          echo "Cleaning intermediate build files to reduce artifact size..."
          # Show disk usage before cleanup
          docker exec viame_build du -sh /viame/build || true

          # Remove object files (keeping only essential build stamps)
          docker exec viame_build find /viame/build -name "*.o" -delete 2>/dev/null || true
          docker exec viame_build find /viame/build -name "*.a" -delete 2>/dev/null || true

          # Remove fletch download cache (sources already extracted/built)
          docker exec viame_build rm -rf /viame/build/build/src/fletch-build/Downloads 2>/dev/null || true

          # Remove CMakeFiles directories (can be regenerated)
          docker exec viame_build find /viame/build -type d -name "CMakeFiles" -exec rm -rf {} + 2>/dev/null || true

          # Clean pip cache
          docker exec viame_build rm -rf /root/.cache/pip 2>/dev/null || true

          # Show disk usage after cleanup
          echo "Disk usage after cleanup:"
          docker exec viame_build du -sh /viame/build || true

      - name: Package Stage 1 artifact
        if: steps.cache-stage1.outputs.cache-hit != 'true'
        run: |
          # Create a compressed archive of the build directory for Stage 2
          echo "Creating Stage 1 artifact..."
          docker exec viame_build bash -c "cd /viame && tar -cf - build | zstd -T0 -3 > /viame/stage1-build.tar.zst"

          # Copy artifact out of container
          mkdir -p stage1-artifacts
          docker cp viame_build:/viame/stage1-build.tar.zst stage1-artifacts/
          docker cp viame_build:/viame/build/build_log_stage1.txt stage1-artifacts/ 2>/dev/null || true

          ls -lh stage1-artifacts/

      - name: Report cache status
        run: |
          if [ "${{ steps.cache-stage1.outputs.cache-hit }}" == "true" ]; then
            echo "Stage 1 cache HIT - skipped build"
            ls -lh stage1-artifacts/
          else
            echo "Stage 1 cache MISS - full build completed"
          fi

      - name: Upload Stage 1 artifact
        uses: actions/upload-artifact@v4
        with:
          name: linux-stage1-build
          path: stage1-artifacts/stage1-build.tar.zst
          retention-days: 1
          compression-level: 0  # Already compressed with zstd

      - name: Upload Stage 1 build log
        uses: actions/upload-artifact@v4
        if: steps.cache-stage1.outputs.cache-hit != 'true'
        with:
          name: linux-stage1-log
          path: stage1-artifacts/build_log_stage1.txt
          retention-days: 7

      - name: Cleanup
        if: always() && steps.cache-stage1.outputs.cache-hit != 'true'
        run: |
          docker stop viame_build || true
          docker rm --force viame_build || true

  build-linux-stage2:
    name: Build Linux Stage 2 (pytorch)
    runs-on: ubuntu-latest
    needs: build-linux-stage1
    timeout-minutes: 480  # 8 hours for pytorch
    outputs:
      cache-hit: ${{ steps.cache-stage2.outputs.cache-hit }}
      cache-key: ${{ steps.cache-key.outputs.key }}

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          submodules: true  # Need submodules to compute hashes

      - name: Compute Stage 2 cache key
        id: cache-key
        run: |
          # Compute hash of files that affect Stage 2 build (pytorch)
          # Include Stage 1 cache key to ensure we rebuild if fletch changes
          STAGE1_KEY="${{ needs.build-linux-stage1.outputs.cache-key }}"
          PYTORCH_PKG_HASH=$(find packages/pytorch -type f \( -name "*.cmake" -o -name "CMakeLists.txt" -o -name "*.patch" -o -name "*.py" -o -name "setup.py" \) -exec sha256sum {} \; 2>/dev/null | sort | sha256sum | cut -c1-16)
          PYTORCH_CMAKE_HASH=$(sha256sum cmake/add_project_pytorch.cmake | cut -c1-16)
          BUILD_SCRIPT_HASH=$(sha256sum cmake/build_server_rocky_stage2.sh 2>/dev/null | cut -c1-16)

          CACHE_KEY="linux-stage2-${STAGE1_KEY}-${PYTORCH_CMAKE_HASH}-${PYTORCH_PKG_HASH}-${BUILD_SCRIPT_HASH}"
          echo "key=$CACHE_KEY" >> $GITHUB_OUTPUT
          echo "Cache key: $CACHE_KEY"

      - name: Check Stage 2 cache
        id: cache-stage2
        uses: actions/cache@v4
        with:
          path: stage2-artifacts/stage2-build.tar.zst
          key: ${{ steps.cache-key.outputs.key }}

      - name: Free disk space
        if: steps.cache-stage2.outputs.cache-hit != 'true'
        run: |
          # Remove unnecessary files to free up disk space (aggressive cleanup for Stage 2)
          sudo rm -rf /usr/share/dotnet
          sudo rm -rf /usr/local/lib/android
          sudo rm -rf /opt/ghc
          sudo rm -rf /opt/hostedtoolcache/CodeQL
          sudo rm -rf /usr/local/share/boost
          sudo rm -rf /usr/share/swift
          sudo rm -rf /usr/local/julia*
          sudo rm -rf /opt/microsoft
          sudo rm -rf /opt/az
          # Additional aggressive cleanup for Stage 2
          sudo rm -rf /opt/hostedtoolcache/Python
          sudo rm -rf /opt/hostedtoolcache/Ruby
          sudo rm -rf /opt/hostedtoolcache/go
          sudo rm -rf /opt/hostedtoolcache/node
          sudo rm -rf /usr/local/graalvm
          sudo rm -rf /usr/share/miniconda
          sudo rm -rf /usr/local/share/chromium
          sudo rm -rf /usr/local/go
          sudo rm -rf /usr/share/rust
          sudo rm -rf /usr/local/.ghcup
          sudo docker image prune --all --force
          df -h

      - name: Download Stage 1 artifact
        if: steps.cache-stage2.outputs.cache-hit != 'true'
        uses: actions/download-artifact@v4
        with:
          name: linux-stage1-build
          path: stage1-artifacts

      - name: Set up Docker
        if: steps.cache-stage2.outputs.cache-hit != 'true'
        run: |
          # Ensure docker is available
          docker --version

      - name: Build Stage 2 in Rocky Linux container
        if: steps.cache-stage2.outputs.cache-hit != 'true'
        run: |
          # Pull the Rocky Linux CUDA image
          docker pull nvidia/cuda:12.6.3-cudnn-devel-rockylinux8

          # Start container (no GPU runtime needed for compilation)
          docker run -td --name viame_build nvidia/cuda:12.6.3-cudnn-devel-rockylinux8 bash

          # Copy source into container
          docker cp . viame_build:/viame/

          # Copy and extract Stage 1 artifact
          echo "Extracting Stage 1 build artifact..."
          docker cp stage1-artifacts/stage1-build.tar.zst viame_build:/viame/
          rm -rf stage1-artifacts  # Free host disk space
          docker exec viame_build yum install -y zstd
          docker exec viame_build bash -c "cd /viame && zstd -d stage1-build.tar.zst -c | tar -xf - && rm stage1-build.tar.zst"

          # Make build script executable
          docker exec viame_build chmod +x /viame/cmake/build_server_rocky_stage2.sh

          # Run the Stage 2 build script
          docker exec viame_build /viame/cmake/build_server_rocky_stage2.sh

      - name: Verify Stage 2 completed successfully
        if: steps.cache-stage2.outputs.cache-hit != 'true'
        run: |
          echo "Checking if Stage 2 build completed successfully..."

          # Check that pytorch was built
          if docker exec viame_build test -d /viame/build/build/src/pytorch-build; then
            echo "SUCCESS: pytorch-build directory found"
          else
            echo "ERROR: pytorch-build directory not found"
            exit 1
          fi

          # Check for install directory
          if docker exec viame_build test -d /viame/build/install; then
            echo "SUCCESS: install directory found"
          else
            echo "ERROR: install directory not found"
            exit 1
          fi

          echo "Stage 2 verification passed!"

      - name: Clean intermediate files before packaging
        if: steps.cache-stage2.outputs.cache-hit != 'true'
        run: |
          echo "Cleaning intermediate build files to reduce artifact size..."
          # Show disk usage before cleanup
          docker exec viame_build du -sh /viame/build || true

          # Remove object files (keeping only essential build stamps)
          docker exec viame_build find /viame/build -name "*.o" -delete 2>/dev/null || true
          docker exec viame_build find /viame/build -name "*.a" -delete 2>/dev/null || true

          # Remove CMakeFiles directories (can be regenerated)
          docker exec viame_build find /viame/build -type d -name "CMakeFiles" -exec rm -rf {} + 2>/dev/null || true

          # Clean pip cache
          docker exec viame_build rm -rf /root/.cache/pip 2>/dev/null || true

          # Show disk usage after cleanup
          echo "Disk usage after cleanup:"
          docker exec viame_build du -sh /viame/build || true

      - name: Package Stage 2 artifact
        if: steps.cache-stage2.outputs.cache-hit != 'true'
        run: |
          # Create a compressed archive of the build directory for Stage 3
          echo "Creating Stage 2 artifact..."
          docker exec viame_build bash -c "cd /viame && tar -cf - build | zstd -T0 -3 > /viame/stage2-build.tar.zst"

          # Copy artifact out of container
          mkdir -p stage2-artifacts
          docker cp viame_build:/viame/stage2-build.tar.zst stage2-artifacts/
          docker cp viame_build:/viame/build/build_log_stage2.txt stage2-artifacts/ 2>/dev/null || true

          ls -lh stage2-artifacts/

      - name: Report cache status
        run: |
          if [ "${{ steps.cache-stage2.outputs.cache-hit }}" == "true" ]; then
            echo "Stage 2 cache HIT - skipped build"
            ls -lh stage2-artifacts/
          else
            echo "Stage 2 cache MISS - full build completed"
          fi

      - name: Upload Stage 2 artifact
        uses: actions/upload-artifact@v4
        with:
          name: linux-stage2-build
          path: stage2-artifacts/stage2-build.tar.zst
          retention-days: 1
          compression-level: 0  # Already compressed with zstd

      - name: Upload Stage 2 build log
        uses: actions/upload-artifact@v4
        if: steps.cache-stage2.outputs.cache-hit != 'true'
        with:
          name: linux-stage2-log
          path: stage2-artifacts/build_log_stage2.txt
          retention-days: 7

      - name: Cleanup
        if: always() && steps.cache-stage2.outputs.cache-hit != 'true'
        run: |
          docker stop viame_build || true
          docker rm --force viame_build || true

  build-linux-stage3:
    name: Build Linux Stage 3 (kwiver + viame)
    runs-on: ubuntu-latest
    needs: build-linux-stage2
    timeout-minutes: 480  # 8 hours for remaining builds

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          submodules: false  # We handle submodules in the build script

      - name: Free disk space
        run: |
          # Remove unnecessary files to free up disk space
          sudo rm -rf /usr/share/dotnet
          sudo rm -rf /usr/local/lib/android
          sudo rm -rf /opt/ghc
          sudo rm -rf /opt/hostedtoolcache/CodeQL
          sudo rm -rf /usr/local/share/boost
          sudo rm -rf /usr/share/swift
          sudo rm -rf /usr/local/julia*
          sudo rm -rf /opt/microsoft
          sudo rm -rf /opt/az
          sudo docker image prune --all --force
          df -h

      - name: Download Stage 2 artifact
        uses: actions/download-artifact@v4
        with:
          name: linux-stage2-build
          path: stage2-artifacts

      - name: Set up Docker
        run: |
          # Ensure docker is available
          docker --version

      - name: Build Stage 3 in Rocky Linux container
        run: |
          # Pull the Rocky Linux CUDA image
          docker pull nvidia/cuda:12.6.3-cudnn-devel-rockylinux8

          # Start container (no GPU runtime needed for compilation)
          docker run -td --name viame_build nvidia/cuda:12.6.3-cudnn-devel-rockylinux8 bash

          # Copy source into container
          docker cp . viame_build:/viame/

          # Copy and extract Stage 2 artifact
          echo "Extracting Stage 2 build artifact..."
          docker cp stage2-artifacts/stage2-build.tar.zst viame_build:/viame/
          rm -rf stage2-artifacts  # Free host disk space
          docker exec viame_build yum install -y zstd
          docker exec viame_build bash -c "cd /viame && zstd -d stage2-build.tar.zst -c | tar -xf - && rm stage2-build.tar.zst"

          # Make build script executable
          docker exec viame_build chmod +x /viame/cmake/build_server_rocky_stage3.sh

          # Run the Stage 3 build script
          docker exec viame_build /viame/cmake/build_server_rocky_stage3.sh

      - name: Verify Linux build completed successfully
        run: |
          echo "Checking if build completed successfully..."

          # Check that setup_viame.sh exists in the install directory
          # The build script renames install to viame for packaging, so check both locations
          if docker exec viame_build test -f /viame/build/install/setup_viame.sh; then
            echo "SUCCESS: setup_viame.sh found in /viame/build/install/"
          elif docker exec viame_build test -f /viame/build/viame/setup_viame.sh; then
            echo "SUCCESS: setup_viame.sh found in /viame/build/viame/"
          else
            echo "ERROR: setup_viame.sh not found in install directory"
            echo "Listing /viame/build/ contents:"
            docker exec viame_build ls -la /viame/build/ || true
            echo "Listing /viame/build/install/ contents (if exists):"
            docker exec viame_build ls -la /viame/build/install/ 2>/dev/null || true
            exit 1
          fi

          # Check that the tarball was created
          if docker exec viame_build find /viame/build -name "VIAME-*-Linux-64Bit.tar.gz" | grep -q .; then
            echo "SUCCESS: Linux tarball found"
            docker exec viame_build find /viame/build -name "VIAME-*-Linux-64Bit.tar.gz"
          else
            echo "ERROR: Linux tarball not found"
            exit 1
          fi

          echo "Linux build verification passed!"

      - name: Extract build artifacts
        run: |
          mkdir -p artifacts
          # Copy the generated tarball (version is extracted from RELEASE_NOTES.md in the script)
          docker cp viame_build:/viame/build/. artifacts/ 2>/dev/null || true
          # Find and copy the tarball
          docker exec viame_build find /viame/build -name "VIAME-*-Linux-64Bit.tar.gz" -exec cat {} \; > artifacts/viame-linux.tar.gz 2>/dev/null || \
          docker cp viame_build:/viame/build/VIAME-v1.0.0-Linux-64Bit.tar.gz artifacts/viame-linux.tar.gz 2>/dev/null || true
          # Also try to get build log for debugging
          docker cp viame_build:/viame/build/build_log_stage3.txt artifacts/build_log_linux.txt 2>/dev/null || true
          ls -la artifacts/

      - name: Get version from release notes
        id: version
        run: |
          VERSION=$(head -n 1 RELEASE_NOTES.md | awk '{print $1}')
          echo "version=$VERSION" >> $GITHUB_OUTPUT

      - name: Rename artifact with version
        run: |
          cd artifacts
          if [ -f viame-linux.tar.gz ]; then
            mv viame-linux.tar.gz VIAME-${{ steps.version.outputs.version }}-Linux-64Bit.tar.gz
          fi
          ls -la

      - name: Upload Linux artifact
        uses: actions/upload-artifact@v4
        with:
          name: viame-linux-release
          path: artifacts/VIAME-*-Linux-64Bit.tar.gz
          retention-days: 7

      - name: Cleanup
        if: always()
        run: |
          docker stop viame_build || true
          docker rm --force viame_build || true

  build-windows:
    name: Build Windows Release
    runs-on: windows-2019  # VS 2019
    timeout-minutes: 720  # 12 hours

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          submodules: false  # We handle submodules in the build script

      - name: Enable long paths on Windows
        run: git config --system core.longpaths true

      - name: Set up CUDA
        uses: Jimver/cuda-toolkit@v0.2.21
        id: cuda-toolkit
        with:
          cuda: '12.8.0'
          method: 'network'

      - name: Download and install cuDNN
        shell: powershell
        run: |
          $CUDA_ROOT = "C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v12.8"
          $cudnnArchive = "C:\temp\cudnn.zip"
          $cudnnUrl = "https://data.kitware.com/api/v1/file/694b27c937f3e31fb199e235/download"
          $expectedMD5 = "b448b131951f661e470e649c96a93ca1"
          $maxRetries = 10
          $retryWaitSeconds = 120  # 2 minutes between retries
          $maxTotalMinutes = 60    # Stop after 1 hour total

          # Create temp directory
          New-Item -ItemType Directory -Force -Path "C:\temp"

          # Download cuDNN with retry logic and MD5 verification
          $downloadSuccess = $false
          $startTime = Get-Date
          for ($attempt = 1; $attempt -le $maxRetries; $attempt++) {
            # Check if we've exceeded the total time limit
            $elapsed = (Get-Date) - $startTime
            if ($elapsed.TotalMinutes -ge $maxTotalMinutes) {
              Write-Host "Exceeded maximum total time of $maxTotalMinutes minutes"
              break
            }

            $remainingMinutes = [math]::Round($maxTotalMinutes - $elapsed.TotalMinutes, 1)
            Write-Host "Downloading cuDNN (attempt $attempt of $maxRetries, $remainingMinutes minutes remaining)..."
            try {
              if (Test-Path $cudnnArchive) { Remove-Item $cudnnArchive -Force }

              # Use WebClient for more reliable large file downloads with progress
              $webClient = New-Object System.Net.WebClient
              $webClient.DownloadFile($cudnnUrl, $cudnnArchive)

              # Check file size
              $fileInfo = Get-Item $cudnnArchive
              Write-Host "Downloaded file size: $($fileInfo.Length) bytes"

              # Verify MD5 hash
              $actualMD5 = (Get-FileHash -Path $cudnnArchive -Algorithm MD5).Hash.ToLower()
              Write-Host "Expected MD5: $expectedMD5"
              Write-Host "Actual MD5:   $actualMD5"

              if ($actualMD5 -eq $expectedMD5) {
                Write-Host "MD5 hash verification passed"
                $downloadSuccess = $true
                break
              } else {
                Write-Host "MD5 hash mismatch (likely incomplete download), will retry..."
              }
            } catch {
              Write-Host "Download failed: $_"
            }

            if ($attempt -lt $maxRetries) {
              Write-Host "Waiting $retryWaitSeconds seconds (2 minutes) before retry..."
              Start-Sleep -Seconds $retryWaitSeconds
            }
          }

          if (-not $downloadSuccess) {
            Write-Error "Failed to download cuDNN after $maxRetries attempts over $([math]::Round($elapsed.TotalMinutes, 1)) minutes"
            exit 1
          }

          # Extract using Expand-Archive (native PowerShell for zip files)
          Write-Host "Extracting cuDNN archive..."
          New-Item -ItemType Directory -Force -Path "C:\temp\cudnn_temp"
          Expand-Archive -Path $cudnnArchive -DestinationPath "C:\temp\cudnn_temp" -Force

          # List what was extracted
          Write-Host "Extracted contents:"
          Get-ChildItem -Path "C:\temp\cudnn_temp" -Recurse -Depth 2 | ForEach-Object { Write-Host $_.FullName }

          # Find the extracted directory (cudnn-windows-x86_64-*-archive)
          $cudnnDir = Get-ChildItem -Path "C:\temp\cudnn_temp" -Directory | Where-Object { $_.Name -match "cudnn" } | Select-Object -First 1
          if (-not $cudnnDir) {
            $cudnnDir = Get-ChildItem -Path "C:\temp\cudnn_temp" -Directory | Select-Object -First 1
          }

          if (-not $cudnnDir) {
            Write-Error "Failed to find cuDNN directory after extraction"
            exit 1
          }

          Write-Host "Found cuDNN directory: $($cudnnDir.FullName)"
          Get-ChildItem -Path $cudnnDir.FullName

          # Verify expected directories exist
          if (-not (Test-Path "$($cudnnDir.FullName)\include")) {
            Write-Error "cuDNN include directory not found - extraction may have failed"
            exit 1
          }

          # Copy cuDNN files directly into CUDA tree
          Write-Host "Installing cuDNN into CUDA directory..."
          Copy-Item -Path "$($cudnnDir.FullName)\bin\*" -Destination "$CUDA_ROOT\bin" -Force
          Copy-Item -Path "$($cudnnDir.FullName)\include\*" -Destination "$CUDA_ROOT\include" -Force
          Copy-Item -Path "$($cudnnDir.FullName)\lib\x64\*" -Destination "$CUDA_ROOT\lib\x64" -Force

          # Verify installation
          Write-Host "cuDNN files installed:"
          Get-ChildItem "$CUDA_ROOT\bin\cudnn*.dll" | ForEach-Object { Write-Host $_.Name }

          # Cleanup
          Remove-Item -Path $cudnnArchive -Force
          Remove-Item -Path "C:\temp\cudnn_temp" -Recurse -Force

      - name: Install dependencies
        shell: powershell
        run: |
          # Install zlib
          choco install zlib -y

          # Create ZLib directory structure expected by build script
          New-Item -ItemType Directory -Force -Path "C:\Program Files\ZLib\dll_x64"

          # Copy zlib DLL to expected location (choco installs to different path)
          $zlibPath = "C:\ProgramData\chocolatey\lib\zlib\tools"
          if (Test-Path "$zlibPath\zlibwapi.dll") {
            Copy-Item -Path "$zlibPath\zlibwapi.dll" -Destination "C:\Program Files\ZLib\dll_x64\" -Force
          } elseif (Test-Path "C:\Windows\System32\zlibwapi.dll") {
            Copy-Item -Path "C:\Windows\System32\zlibwapi.dll" -Destination "C:\Program Files\ZLib\dll_x64\" -Force
          } else {
            # Download zlib DLL directly if not found
            Write-Host "Downloading zlibwapi.dll..."
            Invoke-WebRequest -Uri "https://www.winimage.com/zLibDll/zlib123dllx64.zip" -OutFile "C:\temp\zlib.zip"
            Expand-Archive -Path "C:\temp\zlib.zip" -DestinationPath "C:\temp\zlib_temp" -Force
            Copy-Item -Path "C:\temp\zlib_temp\dll_x64\zlibwapi.dll" -Destination "C:\Program Files\ZLib\dll_x64\" -Force
            Remove-Item -Path "C:\temp\zlib.zip" -Force
            Remove-Item -Path "C:\temp\zlib_temp" -Recurse -Force
          }

          # Install CMake
          choco install cmake --installargs 'ADD_CMAKE_TO_PATH=System' -y

          # Install Git
          choco install git -y

      - name: Set up Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '18'

      - name: Install yarn
        run: npm install -g yarn

      - name: Configure build paths
        shell: powershell
        run: |
          # Create required directories
          New-Item -ItemType Directory -Force -Path "C:\VIAME-Builds\GPU"

          # Copy source to expected location
          Copy-Item -Path ".\*" -Destination "C:\VIAME-Builds\GPU" -Recurse -Force

      - name: Run Windows build
        shell: cmd
        working-directory: C:\VIAME-Builds\GPU
        run: |
          call cmake\build_server_windows.bat

      - name: Verify Windows build completed successfully
        shell: powershell
        run: |
          Write-Host "Checking if build completed successfully..."

          $installPath = "C:\VIAME-Builds\GPU\build\install"
          $viamePath = "C:\VIAME-Builds\GPU\build\VIAME"
          $setupScript = "setup_viame.bat"

          # Check that setup_viame.bat exists in the install directory
          # The build script renames install to VIAME for packaging, so check both locations
          if (Test-Path "$installPath\$setupScript") {
            Write-Host "SUCCESS: $setupScript found in $installPath"
          } elseif (Test-Path "$viamePath\$setupScript") {
            Write-Host "SUCCESS: $setupScript found in $viamePath"
          } else {
            Write-Host "ERROR: $setupScript not found in install directory"
            Write-Host "Listing C:\VIAME-Builds\GPU\build\ contents:"
            Get-ChildItem -Path "C:\VIAME-Builds\GPU\build\" -ErrorAction SilentlyContinue | Format-Table Name, Length, LastWriteTime
            Write-Host "Listing $installPath contents (if exists):"
            Get-ChildItem -Path $installPath -ErrorAction SilentlyContinue | Format-Table Name, Length, LastWriteTime
            exit 1
          }

          # Check that the zip file was created
          $zipFiles = Get-ChildItem -Path "C:\VIAME-Builds\GPU\build\" -Filter "VIAME-*-Windows-64Bit.zip" -ErrorAction SilentlyContinue
          if ($zipFiles) {
            Write-Host "SUCCESS: Windows zip file found:"
            $zipFiles | ForEach-Object { Write-Host $_.FullName }
          } else {
            Write-Host "ERROR: Windows zip file not found"
            exit 1
          }

          Write-Host "Windows build verification passed!"

      - name: Get version from release notes
        id: version
        shell: bash
        run: |
          VERSION=$(head -n 1 RELEASE_NOTES.md | awk '{print $1}')
          echo "version=$VERSION" >> $GITHUB_OUTPUT

      - name: Upload Windows artifact
        uses: actions/upload-artifact@v4
        with:
          name: viame-windows-release
          path: C:\VIAME-Builds\GPU\build\VIAME-*-Windows-64Bit.zip
          retention-days: 7

  create-release:
    name: Create GitHub Release
    needs: [build-linux-stage3, build-windows]
    runs-on: ubuntu-latest
    permissions:
      contents: write

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Get version from release notes
        id: version
        run: |
          VERSION=$(head -n 1 RELEASE_NOTES.md | awk '{print $1}')
          echo "version=$VERSION" >> $GITHUB_OUTPUT

      - name: Download Linux artifact
        uses: actions/download-artifact@v4
        with:
          name: viame-linux-release
          path: ./release-artifacts

      - name: Download Windows artifact
        uses: actions/download-artifact@v4
        with:
          name: viame-windows-release
          path: ./release-artifacts

      - name: List artifacts
        run: ls -la ./release-artifacts

      - name: Delete existing 'current' release
        env:
          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          # Delete the existing 'current' release if it exists
          gh release delete current --yes || true
          # Also delete the tag
          git push origin :refs/tags/current || true

      - name: Create 'current' release
        env:
          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          gh release create current \
            --title "Current Build (${{ steps.version.outputs.version }})" \
            --notes "Automated build from main branch.

          Version: ${{ steps.version.outputs.version }}
          Commit: ${{ github.sha }}
          Built: $(date -u +"%Y-%m-%d %H:%M:%S UTC")

          This release is automatically updated whenever the main branch is updated." \
            --prerelease \
            ./release-artifacts/*
